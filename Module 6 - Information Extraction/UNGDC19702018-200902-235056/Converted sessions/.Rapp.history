library(readtext)#
library(quanteda)#
library(dplyr)#
library(stringr)#
library(ggplot2)#
library(rworldmap)#
library(RColorBrewer)#
library(classInt)#
library(vegan)#
library(boot)#
library(haven)#
library(readxl)#
library(texreg)#
library(randomForest)
if (!require("readtext")) devtools::install_github("kbenoit/readtext")
library(readtext)#
library(quanteda)#
library(dplyr)#
library(stringr)#
library(ggplot2)#
library(rworldmap)#
library(RColorBrewer)#
library(classInt)#
library(vegan)#
library(boot)#
library(haven)#
library(readxl)#
library(texreg)#
library(randomForest)
rm(list = ls(all = TRUE))#
#
library(foreign)#
library(rworldmap)#
library(RColorBrewer)#
library(classInt)#
library(ggplot2)#
library(dplyr)#
library(readr)#
library(quanteda)#
library(vegan)#
library(ca)#
library(tm)#
library(wordcloud)#
library(dfmta)#
library(topicmodels)#
#
library(readtext)#
library(stringr)#
library(boot)#
library(haven)#
library(readxl)#
library(texreg)#
library(randomForest)
DATA_DIR <- "~/Dropbox/Research/UN Data/" #
#
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))
DATA_DIR <- "~/ALEX/Dropbox/UN Data/"
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))
DATA_DIR <- "ALEX/Dropbox/UN Data/" #
#
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))
DATA_DIR <- "ALEX/Dropbox/UN Data/"
DATA_DIR <- "/ALEX/Dropbox/UN Data/" #
#
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))
#changing row.names to have only country_year, rather than folder pathway from `readtext`.#
row.names(ungd_files) <- str_replace(str_replace(sapply(str_split(row.names(ungd_files), "/"),`[`,2), ".txt", ""), "_\\d{2}", "")#
#
if (!require("quanteda")) devtools::install_github("kbenoit/quanteda")#
#
ungd_corpus <- corpus(ungd_files, text_field = "text")
Creating document feature matrix.#
#
```{r, include=FALSE}#
#Tokenization and basic pre-processing#
tok <- tokens(ungd_corpus, what = "word",#
              removePunct = TRUE,#
              removeSymbols = TRUE,#
              removeNumbers = TRUE,#
              removeTwitter = TRUE,#
              removeURL = TRUE,#
              removeHyphens = TRUE,#
              verbose = TRUE)
#DFM creation from tokens, removing stopwords, and stemming.#
dfm <- dfm(tok, #
           tolower = TRUE,#
           remove=stopwords("SMART"),#
           stem=TRUE, #
           verbose = TRUE)#
#
#Showing 100 most frequrent tokens in DFM#
topfeatures(dfm, n = 100)
```{r}#
#Removing any digits. `dfm` picks up any separated digits, not digits that are part of tokens.#
#Removing any punctuation. `dfm` picks up any punctuation unless it's part of a token.#
#Removing any tokens less than four characters.#
dfm.m <- dfm_select(dfm, c("[\\d-]", "[[:punct:]]", "^.{1,3}$"), selection = "remove", #
                    valuetype="regex", verbose = TRUE)#
#
#100 least frequent terms#
topfeatures(dfm.m, n = 100, decreasing = FALSE)#
```#
```{r, include=FALSE}#
#Dropping words that appear less than 5 times and in less than 3 documents.#
dfm.trim <- dfm_trim(dfm.m, min_count = 5, min_docfreq = 3)#
#
#100 least frequent terms in trimmed DFM#
topfeatures(dfm.trim, n = 100, decreasing = FALSE)#
#
#Level of sparsity of trimmed DFM#
sparsity(dfm.trim)#
#
#Number of features in trimmed DFM#
nfeature(dfm.trim)#
```
```{r tfidf}#
#
dfm.w <- dfm_weight(dfm.trim, type = "tfidf")#
#
```#
##Wordfish#
```{r}#
usa <- which(ungd_files$Country == "USA" & ungd_files$Year == 1971)#
rus <- which(ungd_files$Country == "RUS" & ungd_files$Year == 1971)#
```
```{r}#
#on iMac: elapsed time 1742.210 sec#
#on MacBook: elapsed time 2602.798 sec#
system.time(wordfish <- textmodel_wordfish(dfm.w, dir = c(rus,usa), dispersion = "poisson"))#
```
```{r}#
thetas <- as.data.frame(wordfish@theta, wordfish@docs)#
thetas <- cbind(str_split(row.names(thetas), "_", simplify = TRUE), thetas)#
names(thetas)[1] <- "Country"#
names(thetas)[2] <- "Year"#
names(thetas)[3] <- "wordfish"#
thetas$Year <- as.numeric(as.character(thetas$Year))#
thetas$Country <- as.character(thetas$Country)#
#
```
DATA_DIR <- "/ALEX/Dropbox/UN Data/" #
#
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))#
#changing row.names to have only country_year, rather than folder pathway from `readtext`.#
row.names(ungd_files) <- str_replace(str_replace(sapply(str_split(row.names(ungd_files), "/"),`[`,2), ".txt", ""), "_\\d{2}", "")#
#
if (!require("quanteda")) devtools::install_github("kbenoit/quanteda")#
#
ungd_corpus <- corpus(ungd_files, text_field = "text") #
#
```#
#
Creating document feature matrix.#
#
```{r, include=FALSE}#
#Tokenization and basic pre-processing#
tok <- tokens(ungd_corpus, what = "word",#
              removePunct = TRUE,#
              removeSymbols = TRUE,#
              removeNumbers = TRUE,#
              removeTwitter = TRUE,#
              removeURL = TRUE,#
              removeHyphens = TRUE,#
              verbose = TRUE)#
```#
#
```{r dfm}#
#DFM creation from tokens, removing stopwords, and stemming.#
dfm <- dfm(tok, #
           tolower = TRUE,#
           remove=stopwords("SMART"),#
           stem=TRUE, #
           verbose = TRUE)#
#
#Showing 100 most frequrent tokens in DFM#
topfeatures(dfm, n = 100)#
#
```#
```{r}#
#Removing any digits. `dfm` picks up any separated digits, not digits that are part of tokens.#
#Removing any punctuation. `dfm` picks up any punctuation unless it's part of a token.#
#Removing any tokens less than four characters.#
dfm.m <- dfm_select(dfm, c("[\\d-]", "[[:punct:]]", "^.{1,3}$"), selection = "remove", #
                    valuetype="regex", verbose = TRUE)#
#
#100 least frequent terms#
topfeatures(dfm.m, n = 100, decreasing = FALSE)#
```#
```{r, include=FALSE}#
#Dropping words that appear less than 5 times and in less than 3 documents.#
dfm.trim <- dfm_trim(dfm.m, min_count = 5, min_docfreq = 3)#
#
#100 least frequent terms in trimmed DFM#
topfeatures(dfm.trim, n = 100, decreasing = FALSE)#
#
#Level of sparsity of trimmed DFM#
sparsity(dfm.trim)#
#
#Number of features in trimmed DFM#
nfeature(dfm.trim)#
```#
```{r tfidf}#
#
dfm.w <- dfm_weight(dfm.trim, type = "tfidf")#
#
```#
##Wordfish#
```{r}#
usa <- which(ungd_files$Country == "USA" & ungd_files$Year == 1971)#
rus <- which(ungd_files$Country == "RUS" & ungd_files$Year == 1971)#
```
```{r wordscoreRUSA, include=FALSE}#
#Wordscore estimations by year#
rusa <- data.frame()#
for (i in 1971:2016) {#
#Creating corpus for 2014, for Wordscore example below#
ungdc.i <- corpus_subset(ungd_corpus, Year==i)#
#
tok <- tokens(ungdc.i, what = "word",#
              removePunct = TRUE,#
              removeSymbols = TRUE,#
              removeNumbers = TRUE,#
              removeTwitter = TRUE,#
              removeURL = TRUE,#
              removeHyphens = TRUE,#
              verbose = TRUE)#
#
dfm <- dfm(tok, #
           tolower = TRUE,#
           remove=stopwords("SMART"),#
           stem=TRUE, #
           verbose = TRUE)#
#
#Removing any digits. `dfm` picks up any separated digits, not digits that are part of tokens.#
#Removing any punctuation. `dfm` picks up any punctuation unless it's part of a token.#
#Removing any tokens less than four characters.#
dfm.m <- dfm_select(dfm, c("[\\d-]", "[[:punct:]]", "^.{1,3}$"), selection = "remove", #
                    valuetype="regex", verbose = TRUE)#
#Dropping words that appear less than 5 times and in less than 3 documents.#
dfm.trim <- dfm_trim(dfm.m, min_count = 5, min_docfreq = 3)#
#
#tfidf weighting#
dfm.w <- quanteda::dfm_weight(dfm.trim, type = "tfidf")#
#Reference scores#
refscores <- rep(NA,nrow(dfm.w))#
#
refscores[str_detect(rownames(dfm.w), "RUS")] <- -1#
refscores[str_detect(rownames(dfm.w), "USA")] <- 1#
#
#Wordscore model#
ws <- textmodel_wordscores(dfm.w, refscores, scale="linear", smooth=1)#
wordscore <- predict(ws, rescaling="none")#
#
#Writing the results into data frame#
wordscores.i <- data.frame(cbind(docvars(ungdc.i), wordscore@textscores$textscore_raw))#
#
wordscores.i <- dplyr::rename(wordscores.i, wscore = wordscore.textscores.textscore_raw)#
#
rusa <- rbind(rusa,wordscores.i)#
#
}#
```
rm(list = ls(all = TRUE))#
#
library(foreign)#
library(rworldmap)#
library(RColorBrewer)#
library(classInt)#
library(ggplot2)#
library(dplyr)#
library(readr)#
library(quanteda)#
library(vegan)#
library(ca)#
library(tm)#
library(wordcloud)#
library(dfmta)#
library(topicmodels)#
library(readtext)#
library(stringr)#
library(boot)#
library(haven)#
library(readxl)#
library(texreg)#
library(randomForest)#
DATA_DIR <- "/ALEX/Dropbox/UN Data/" #
#
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))#
#changing row.names to have only country_year, rather than folder pathway from `readtext`.#
row.names(ungd_files) <- str_replace(str_replace(sapply(str_split(row.names(ungd_files), "/"),`[`,2), ".txt", ""), "_\\d{2}", "")#
#
if (!require("quanteda")) devtools::install_github("kbenoit/quanteda")#
#
ungd_corpus <- corpus(ungd_files, text_field = "text") #
#
```#
#
Creating document feature matrix.#
#
```{r, include=FALSE}#
#Tokenization and basic pre-processing#
tok <- tokens(ungd_corpus, what = "word",#
              removePunct = TRUE,#
              removeSymbols = TRUE,#
              removeNumbers = TRUE,#
              removeTwitter = TRUE,#
              removeURL = TRUE,#
              removeHyphens = TRUE,#
              verbose = TRUE)#
```#
#
```{r dfm}#
#DFM creation from tokens, removing stopwords, and stemming.#
dfm <- dfm(tok, #
           tolower = TRUE,#
           remove=stopwords("SMART"),#
           stem=TRUE, #
           verbose = TRUE)#
#
#Showing 100 most frequrent tokens in DFM#
topfeatures(dfm, n = 100)#
#
```#
```{r}#
#Removing any digits. `dfm` picks up any separated digits, not digits that are part of tokens.#
#Removing any punctuation. `dfm` picks up any punctuation unless it's part of a token.#
#Removing any tokens less than four characters.#
dfm.m <- dfm_select(dfm, c("[\\d-]", "[[:punct:]]", "^.{1,3}$"), selection = "remove", #
                    valuetype="regex", verbose = TRUE)#
#
#100 least frequent terms#
topfeatures(dfm.m, n = 100, decreasing = FALSE)#
```#
```{r, include=FALSE}#
#Dropping words that appear less than 5 times and in less than 3 documents.#
dfm.trim <- dfm_trim(dfm.m, min_count = 5, min_docfreq = 3)#
#
#100 least frequent terms in trimmed DFM#
topfeatures(dfm.trim, n = 100, decreasing = FALSE)#
#
#Level of sparsity of trimmed DFM#
sparsity(dfm.trim)#
#
#Number of features in trimmed DFM#
nfeature(dfm.trim)#
```#
```{r tfidf}#
#
dfm.w <- dfm_weight(dfm.trim, type = "tfidf")#
#
```#
##Wordfish#
```{r}#
usa <- which(ungd_files$Country == "USA" & ungd_files$Year == 1971)#
rus <- which(ungd_files$Country == "RUS" & ungd_files$Year == 1971)#
```
setwd("/ALEX/Dropbox/UN General Debate/Analysis/input")
DATA_DIR <- "/ALEX/Dropbox/UN Data/" #
#
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))
warnings()
row.names(ungd_files) <- str_replace(str_replace(sapply(str_split(row.names(ungd_files), "/"),`[`,2), ".txt", ""), "_\\d{2}", "")
ungd_corpus <- corpus(ungd_files, text_field = "text")
tok <- tokens(ungd_corpus, what = "word",#
              removePunct = TRUE,#
              removeSymbols = TRUE,#
              removeNumbers = TRUE,#
              removeTwitter = TRUE,#
              removeURL = TRUE,#
              removeHyphens = TRUE,#
              verbose = TRUE)
dfm <- dfm(tok, #
           tolower = TRUE,#
           remove=stopwords("SMART"),#
           stem=TRUE, #
           verbose = TRUE)
dfm.m <- dfm_select(dfm, c("[\\d-]", "[[:punct:]]", "^.{1,3}$"), selection = "remove", #
                    valuetype="regex", verbose = TRUE)
dfm.trim <- dfm_trim(dfm.m, min_count = 5, min_docfreq = 3)#
#
#100 least frequent terms in trimmed DFM#
topfeatures(dfm.trim, n = 100, decreasing = FALSE)#
#
#Level of sparsity of trimmed DFM#
sparsity(dfm.trim)#
#
#Number of features in trimmed DFM#
nfeature(dfm.trim)
dfm.w <- dfm_weight(dfm.trim, type = "tfidf")
usa <- which(ungd_files$Country == "USA" & ungd_files$Year == 1971)#
rus <- which(ungd_files$Country == "RUS" & ungd_files$Year == 1971)
system.time(wordfish <- textmodel_wordfish(dfm.w, dir = c(rus,usa), dispersion = "poisson"))
thetas <- as.data.frame(wordfish@theta, wordfish@docs)#
thetas <- cbind(str_split(row.names(thetas), "_", simplify = TRUE), thetas)#
names(thetas)[1] <- "Country"#
names(thetas)[2] <- "Year"#
names(thetas)[3] <- "wordfish"#
thetas$Year <- as.numeric(as.character(thetas$Year))#
thetas$Country <- as.character(thetas$Country)
thetas
scores.wordfish <- data.frame(cbind(docvars(un.corpus),#
                                    thetas))
scores.wordfish <- data.frame(cbind(docvars(ungd_files),#
                                    thetas))
getwd()
write.csv(scores.wordfish,file="scores-wordfish.csv")
rusa <- data.frame()#
for (i in 1971:2016) {#
#Creating corpus for 2014, for Wordscore example below#
ungdc.i <- corpus_subset(ungd_corpus, Year==i)#
#
tok <- tokens(ungdc.i, what = "word",#
              removePunct = TRUE,#
              removeSymbols = TRUE,#
              removeNumbers = TRUE,#
              removeTwitter = TRUE,#
              removeURL = TRUE,#
              removeHyphens = TRUE,#
              verbose = TRUE)#
#
dfm <- dfm(tok, #
           tolower = TRUE,#
           remove=stopwords("SMART"),#
           stem=TRUE, #
           verbose = TRUE)
stop
sto()
stop()
dtm.us.coldwar <- dfm(subset(dfm.trim, Year>=1971 & Year<1992 & Country=="USA"), #
           ignoredFeatures= c("united", "nations", stopwords("SMART")), #
           stem=TRUE, removeNumbers=TRUE, removePunct = TRUE,#
removeSymbols = TRUE, removeHyphens = TRUE,#
verbose = TRUE,#
           bigrams=FALSE, #
           language="english")
)
dtm.us.coldwar <- dfm(subset(dfm.trim, Year>=1971 & Year<1992 & Country=="USA"), #
           ignoredFeatures= c("united", "nations", stopwords("SMART")), #
           stem=TRUE, removeNumbers=TRUE, removePunct = TRUE,#
removeSymbols = TRUE, removeHyphens = TRUE,#
verbose = TRUE,#
           bigrams=FALSE, #
           language="english")
dtm.us.coldwar <- dfm(subset(dfm.trim, ungd_files$Year>=1971 & ungd_files$Year<1992 & ungd_files$Country=="USA"), #
           ignoredFeatures= c("united", "nations", stopwords("SMART")), #
           stem=TRUE, removeNumbers=TRUE, removePunct = TRUE,#
removeSymbols = TRUE, removeHyphens = TRUE,#
verbose = TRUE,#
           bigrams=FALSE, #
           language="english")
out.file <- paste("coldwar.pdf",sep="")#
pdf(file=out.file)#
plot(reduced.dtm.un.coldwar, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35) + title("USA during the Cold War")#
dev.off()
getwd()
dtm.us.coldwar
out.file <- paste("coldwar.pdf",sep="")#
pdf(file=out.file)#
plot(dtm.us.coldwar, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35) + title("USA during the Cold War")#
dev.off()
topfeatures(dtm.us.coldwar,1000,decreasing=FALSE)
topfeatures(dtm.us.coldwar,25,decreasing=FALSE)
dtm.us.coldwar <- dfm(subset(dfm.trim, ungd_files$Year>=1971 & ungd_files$Year<1992 & ungd_files$Country=="USA"), #
           ignoredFeatures= c("united", "nations", stopwords("SMART")), #
           stem=TRUE, removeNumbers=TRUE, removePunct = TRUE,#
removeSymbols = TRUE, removeHyphens = TRUE,#
verbose = TRUE,#
           bigrams=FALSE, #
           language="english")
rm(list = ls(all = TRUE))#
#
library(foreign)#
library(rworldmap)#
library(RColorBrewer)#
library(classInt)#
library(ggplot2)#
library(dplyr)#
library(readr)#
library(quanteda)#
library(vegan)#
library(ca)#
library(tm)#
library(wordcloud)#
library(dfmta)#
library(topicmodels)#
library(readtext)#
library(stringr)#
library(boot)#
library(haven)#
library(readxl)#
library(texreg)#
library(randomForest)
dtm.us.coldwar <- dfm(subset(dfm.trim, ungd_files$Year>=1971 & ungd_files$Year<1992 & ungd_files$Country=="USA"), #
           ignoredFeatures= c("united", "nations", stopwords("SMART")), #
           stem=TRUE, removeNumbers=TRUE, removePunct = TRUE,#
removeSymbols = TRUE, removeHyphens = TRUE,#
verbose = TRUE,#
           bigrams=FALSE, #
           language="english") #
  out.file <- paste("coldwar.pdf",sep="")#
pdf(file=out.file)#
plot(dtm.us.coldwar, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35) + title("USA during the Cold War")#
dev.off()
DATA_DIR <- "/ALEX/Dropbox/UN Data/" #
#
ungd_files <- readtext(paste0(DATA_DIR, "Converted sessions/*"), #
                                 docvarsfrom = "filenames", #
                                 dvsep="_", #
                                 docvarnames = c("Country", "Session", "Year"))#
#changing row.names to have only country_year, rather than folder pathway from `readtext`.#
row.names(ungd_files) <- str_replace(str_replace(sapply(str_split(row.names(ungd_files), "/"),`[`,2), ".txt", ""), "_\\d{2}", "")#
#
if (!require("quanteda")) devtools::install_github("kbenoit/quanteda")#
#
ungd_corpus <- corpus(ungd_files, text_field = "text") #
#
```#
#
Creating document feature matrix.#
#
```{r, include=FALSE}#
#Tokenization and basic pre-processing#
tok <- tokens(ungd_corpus, what = "word",#
              removePunct = TRUE,#
              removeSymbols = TRUE,#
              removeNumbers = TRUE,#
              removeTwitter = TRUE,#
              removeURL = TRUE,#
              removeHyphens = TRUE,#
              verbose = TRUE)#
```#
#
```{r dfm}#
#DFM creation from tokens, removing stopwords, and stemming.#
dfm <- dfm(tok, #
           tolower = TRUE,#
           remove=stopwords("SMART"),#
           stem=TRUE, #
           verbose = TRUE)#
#
#Showing 100 most frequrent tokens in DFM#
topfeatures(dfm, n = 100)#
#
```#
```{r}#
#Removing any digits. `dfm` picks up any separated digits, not digits that are part of tokens.#
#Removing any punctuation. `dfm` picks up any punctuation unless it's part of a token.#
#Removing any tokens less than four characters.#
dfm.m <- dfm_select(dfm, c("[\\d-]", "[[:punct:]]", "^.{1,3}$"), selection = "remove", #
                    valuetype="regex", verbose = TRUE)#
#
#100 least frequent terms#
topfeatures(dfm.m, n = 100, decreasing = FALSE)#
```#
```{r, include=FALSE}#
#Dropping words that appear less than 5 times and in less than 3 documents.#
dfm.trim <- dfm_trim(dfm.m, min_count = 5, min_docfreq = 3)#
#
#100 least frequent terms in trimmed DFM#
topfeatures(dfm.trim, n = 100, decreasing = FALSE)#
#
#Level of sparsity of trimmed DFM#
sparsity(dfm.trim)#
#
#Number of features in trimmed DFM#
nfeature(dfm.trim)#
```#
```{r tfidf}#
#
dfm.w <- dfm_weight(dfm.trim, type = "tfidf")
